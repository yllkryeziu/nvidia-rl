defaults: ../../distillation_math.yaml
distillation:
  num_prompts_per_step: 64
  max_num_steps: 20
  val_batch_size: 32
  val_period: 10
  max_val_samples: 256
loss_fn:
  kl_type: reverse
checkpointing:
  checkpoint_dir: checkpoints/distillation-qwen3-32b-to-1.7b-base
policy:
  model_name: /p/project1/envcomp/yll/.cache/huggingface/hub/models--Qwen--Qwen3-1.7B-Base/snapshots/ea980cb0a6c2ae4b936e82123acc929f1cec04c1
  train_global_batch_size: 32
  generation_batch_size: 32
  dtensor_cfg:
    tensor_parallel_size: 1
    context_parallel_size: 1
  dynamic_batching:
    enabled: false
  make_sequence_length_divisible_by: 1
  scheduler:
  - name: torch.optim.lr_scheduler.LinearLR
    kwargs:
      start_factor: 0.1
      end_factor: 1.0
      total_iters: 20
  - name: torch.optim.lr_scheduler.ConstantLR
    kwargs:
      factor: 1.0
      total_iters: 10000000000
  - milestones:
    - 20
teacher:
  model_name: /p/project1/envcomp/yll/.cache/huggingface/hub/models--Qwen--Qwen3-32B/snapshots/9216db5781bf21249d130ec9da846c4624c16137
  train_global_batch_size: 32
  generation_batch_size: 32
  dtensor_cfg:
    context_parallel_size: 1
  dynamic_batching:
    enabled: false
  make_sequence_length_divisible_by: 1
  scheduler:
  - name: torch.optim.lr_scheduler.LinearLR
    kwargs:
      start_factor: 0.1
      end_factor: 1.0
      total_iters: 20
  - name: torch.optim.lr_scheduler.ConstantLR
    kwargs:
      factor: 1.0
      total_iters: 10000000000
  - milestones:
    - 20
logger:
  log_dir: logs/distillation-qwen3-32b-to-1.7b-base
  wandb:
    project: nemo-rl
    name: distillation-qwen3-32b-to-1.7b-base
