defaults: ../../distillation_math.yaml

distillation:
  num_prompts_per_step: 8
  max_num_steps: 100
  val_batch_size: 16
  val_period: 20
  max_val_samples: 64

checkpointing:
  checkpoint_dir: checkpoints/my-distillation-1n4g

policy:
  model_name: /p/project1/envcomp/yll/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca
  train_global_batch_size: 8
  generation_batch_size: 8
  max_total_sequence_length: 4096
  dtensor_cfg:
    tensor_parallel_size: 1
    context_parallel_size: 1
    activation_checkpointing: true
  make_sequence_length_divisible_by: 2
  generation:
    vllm_cfg:
      tensor_parallel_size: 1
      gpu_memory_utilization: 0.5
      max_model_len: 4096
    colocated:
      enabled: true
      resources:
        gpus_per_node: null
        num_nodes: null

teacher:
  model_name: /p/project1/envcomp/yll/.cache/huggingface/hub/models--Qwen--Qwen3-4B/snapshots/1cfa9a7208912126459214e8b04321603b3df60c
  train_global_batch_size: 8
  generation_batch_size: 8
  max_total_sequence_length: 4096
  dtensor_cfg:
    tensor_parallel_size: 1
    context_parallel_size: 1
    activation_checkpointing: true
  make_sequence_length_divisible_by: 2

cluster:
  gpus_per_node: 4
  num_nodes: 1

logger:
  log_dir: logs/my-distillation-1n4g
  wandb_enabled: false
  wandb:
    project: nemo-distillation
    name: my-distillation-1n4g
