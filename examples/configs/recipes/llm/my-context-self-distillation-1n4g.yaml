defaults: ../../distillation_math.yaml

distillation:
  num_prompts_per_step: 8
  max_num_steps: 100
  val_batch_size: 16
  val_period: 20
  max_val_samples: 64
  max_rollout_turns: 1
  context_distillation:
    enabled: true
    mode: self_frozen
    problem_source: original_user_problem
    teacher_prefix_template: "You are given a problem and a trace: {problem} + {trace}. Solve it on your own."
    alignment: response_token_index
    overflow_policy: truncate_prefix_only
    frozen_teacher_source: base_model
    trace_extractor:
      type: think_tag
      selection: first
      missing_trace_policy: empty
    metrics:
      enabled: true

checkpointing:
  checkpoint_dir: checkpoints/my-context-self-distillation-1n4g

policy:
  model_name: /p/project1/envcomp/yll/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca
  train_global_batch_size: 8
  generation_batch_size: 8
  max_total_sequence_length: 4096
  dtensor_cfg:
    tensor_parallel_size: 1
    context_parallel_size: 1
    activation_checkpointing: true
  make_sequence_length_divisible_by: 2
  generation:
    vllm_cfg:
      tensor_parallel_size: 1
      gpu_memory_utilization: 0.5
      max_model_len: 4096
    colocated:
      enabled: true
      resources:
        gpus_per_node: null
        num_nodes: null

teacher:
  model_name: /p/project1/envcomp/yll/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca
  train_global_batch_size: 8
  generation_batch_size: 8
  max_total_sequence_length: 4096
  dtensor_cfg:
    tensor_parallel_size: 1
    context_parallel_size: 1
    activation_checkpointing: true
  make_sequence_length_divisible_by: 2

cluster:
  gpus_per_node: 4
  num_nodes: 1

logger:
  log_dir: logs/my-context-self-distillation-1n4g
  wandb_enabled: false
  wandb:
    project: nemo-distillation
    name: my-context-self-distillation-1n4g
